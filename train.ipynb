{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.9 | packaged by conda-forge | (main, Mar  4 2025, 22:37:18) [MSC v.1943 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\D\\Anaconda\\envs\\cuda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU:  1\n",
      "GPU Name:  NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import os\n",
    "\n",
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name())\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Configure Model, Tokenizer, and Quantization (QLoRA) ---\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# Path or name of your dataset. The dataset should have a column 'text' formatted with LLaMA chat template\n",
    "# Example:\n",
    "# \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe capital of France is Paris.<|eot_id|>\"\n",
    "dataset_name = \"sample_dataset.jsonl\"  # <<< CHANGE THIS\n",
    "output_dir = \"./results/llama3-8b-finetuned-results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:19<00:00,  4.79s/it]\n"
     ]
    }
   ],
   "source": [
    "# QLoRA quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 167,772,160 || all params: 8,198,033,408 || trainable%: 2.0465\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Prepare Model for QLoRA Training ---\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# --- 3. LoRA Configuration ---\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "# Apply PEFT with LoRA\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n",
    "\n",
    "# --- 4. Load and Prepare Dataset ---\n",
    "try:\n",
    "    dataset = load_dataset(\"json\", data_files={\"train\": \"sample_dataset.jsonl\"}, split=\"train\")\n",
    "    dataset = dataset.shuffle(seed=42)\n",
    "    split_dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    train_dataset = split_dataset[\"train\"]\n",
    "    eval_dataset = split_dataset[\"test\"]\n",
    "except:\n",
    "    dataset = load_dataset(\"json\", data_files={\"train\": dataset_name}, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\D\\Anaconda\\envs\\cuda\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\D\\Anaconda\\envs\\cuda\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='52' max='52' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [52/52 37:41, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.845900</td>\n",
       "      <td>1.241946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.221900</td>\n",
       "      <td>0.919839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.542700</td>\n",
       "      <td>0.805013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\D\\Anaconda\\envs\\cuda\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "g:\\D\\Anaconda\\envs\\cuda\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "g:\\D\\Anaconda\\envs\\cuda\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "Saving fine-tuned adapter model to ./results/llama3-8b-finetuned-results\n",
      "Adapter and tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Training Arguments Configuration ---\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    save_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"tensorboard\",\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=1024,\n",
    "    packing=False,\n",
    "    num_train_epochs=4,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # V√¨ ƒë√¢y l√† causal LM (kh√¥ng d√πng masked LM nh∆∞ BERT)\n",
    ")\n",
    "\n",
    "from transformers.trainer_callback import TrainerCallback\n",
    "\n",
    "class MemoryClearCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, **kwargs):\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "# --- 6. Initialize SFTTrainer ---\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    peft_config=peft_config,\n",
    "    args=sft_config,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "trainer.add_callback(MemoryClearCallback())\n",
    "# --- 7. Start Training ---\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# --- 8. Save Adapter and Tokenizer ---\n",
    "print(f\"Saving fine-tuned adapter model to {output_dir}\")\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"Adapter and tokenizer saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Optional: Clean Up Memory ---\n",
    "# del model\n",
    "# del trainer\n",
    "# import gc\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing Inference with Fine-tuned Model ---\n"
     ]
    }
   ],
   "source": [
    "# --- 9. Inference with Fine-tuned Model ---\n",
    "print(\"\\n--- Testing Inference with Fine-tuned Model ---\")\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Inference Result ---\n",
      "T√°c ph·∫©m 'Chi·∫øc thuy·ªÅn ngo√†i xa' l√† m·ªôt t√°c ph·∫©m ti√™u bi·ªÉu c·ªßa vƒÉn h·ªçc Vi·ªát Nam th·ªùi k·ª≥ ƒë·ªïi m·ªõi, mang ƒë·∫≠m t√≠nh tri·∫øt l√Ω nh√¢n sinh. N√≥ ƒë·∫∑t ra v·∫•n ƒë·ªÅ v·ªÅ s·ª± ƒë·ªëi l·∫≠p gi·ªØa ngh·ªá thu·∫≠t v√† hi·ªán th·ª±c, c≈©ng nh∆∞ c√°i nh√¨n ƒëa chi·ªÅu v·ªÅ con ng∆∞·ªùi v√† cu·ªôc s·ªëng. Qua ƒë√≥, t√°c ph·∫©m g·ª£i m·ªü m·ªôt s·ªë thought v·ªÅ c√°i ƒë·∫πp, s·ª± th·∫≠t v√† tr√°ch nhi·ªám c·ªßa con ng∆∞·ªùi. T√≥m l·∫°i, 'Chi·∫øc thuy·ªÅn ngo√†i xa' l√† m·ªôt t√°c ph·∫©m c√≥ gi√° tr·ªã tri·∫øt l√Ω s√¢u s·∫Øc, g√≥p ph·∫ßn v√†o s·ª± ph√°t tri·ªÉn c·ªßa vƒÉn h·ªçc Vi·ªát Nam th·ªùi k·ª≥ ƒë·ªïi m·ªõi.\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "# Test prompt\n",
    "user_input_test = \"Gi·ªõi thi·ªáu n·ªôi dung tri·∫øt l√Ω trong t√°c ph·∫©m 'Chi·∫øc thuy·ªÅn ngo√†i xa'.\"\n",
    "system_content_test = \"You are an expert in analyzing and interpreting philosophical aspects in literature, with a focus on Vietnamese literary works. **Provide your response in Vietnamese.**\"\n",
    "\n",
    "messages_test = [\n",
    "    {\"role\": \"system\", \"content\": system_content_test},\n",
    "    {\"role\": \"user\", \"content\": user_input_test},\n",
    "]\n",
    "\n",
    "prompt_test = tokenizer.apply_chat_template(messages_test, tokenize=False, add_generation_prompt=True)\n",
    "result = pipe(prompt_test)\n",
    "\n",
    "generated_full = result[0]['generated_text']\n",
    "answer = generated_full[len(prompt_test):].strip()\n",
    "\n",
    "print(\"\\n--- Inference Result ---\")\n",
    "print(answer)\n",
    "print(\"----------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
