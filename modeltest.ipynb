{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\D\\Anaconda\\envs\\cuda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPU:  1\n",
      "GPU Name:  NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "import torch\n",
    "import os\n",
    "# *** Thêm thư viện nhận diện ngôn ngữ ***\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "\n",
    "print(\"Number of GPU: \", torch.cuda.device_count())\n",
    "print(\"GPU Name: \", torch.cuda.get_device_name())\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",             # nf4: tốt nhất cho inference\n",
    "    bnb_4bit_use_double_quant=True,        # giúp giảm lỗi số học\n",
    "    bnb_4bit_compute_dtype=torch.float16   # giảm VRAM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:17<00:00,  4.40s/it]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",  # Đưa model lên GPU\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: vi\n",
      "System Prompt: You are an expert in analyzing and interpreting philosophical aspects in literature, with a focus on Vietnamese literary works. **Provide your response in Vietnamese.**\n"
     ]
    }
   ],
   "source": [
    "# Detect lang and setup input\n",
    "\n",
    "user_input = \"Giới thiệu nội dung triết lý trong tác phẩm 'Chiếc thuyền ngoài xa'.\"\n",
    "system_content_base = \"You are an expert in analyzing and interpreting philosophical aspects in literature, with a focus on Vietnamese literary works.\"\n",
    "\n",
    "detected_lang = \"vi\"\n",
    "\n",
    "try:\n",
    "    detected_lang = detect(user_input)\n",
    "    print(f\"Detected language: {detected_lang}\")\n",
    "except LangDetectException:\n",
    "    print(\"Could not detect language, defaulting to English.\")\n",
    "\n",
    "response_language = \"Vietnamese\" if detected_lang == \"vi\" else \"English\"\n",
    "\n",
    "system_content = f\"{system_content_base} **Provide your response in {response_language}.**\"\n",
    "print(f\"System Prompt: {system_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Response ---\n",
      "Tác phẩm \"Chiếc thuyền ngoài xa\" của Nguyễn Nhật Anh là một trong những tiểu thuyết được đánh giá cao về mặt triết lý và ý nghĩa văn hóa. Tác phẩm này khám phá những vấn đề triết lý quan trọng về cuộc sống, tử thần, tình yêu, trách nhiệm và trách nhiệm cá nhân.\n",
      "\n",
      "Về mặt triết lý, \"Chiếc thuyền ngoài xa\" có thể được hiểu là một cuộc phiêu lưu tìm kiếm ý nghĩa của cuộc sống. Nhân vật chính, Hằng, là một cô gái trẻ sống trong một xã hội truyền thống, nhưng cô lại có một niềm tin mạnh mẽ về cuộc sống và tử thần. Cô cho rằng, cuộc sống là một chuyến phiêu lưu dài, và mỗi người đều phải tự tìm kiếm ý nghĩa của cuộc sống mình.\n",
      "\n",
      "Tác phẩm cũng khám phá vấn đề về tử thần và trách nhiệm. Nhân vật chính phải đối mặt với tử thần của người thân, và phải tìm cách để vượt qua đau khổ và chấp nhận thực tế. Điều này có thể được hiểu là một cuộc đấu tranh để tìm kiếm ý nghĩa của cuộc sống, và chấp nhận rằng cuộc sống là ngắn ngủi và vô nghĩa.\n",
      "\n",
      "Tình yêu cũng là một vấn đề triết lý quan trọng trong tác phẩm. Hằng và bạn trai, Minh, có một tình yêu mạnh mẽ, nhưng họ lại phải đối mặt với nhiều khó khăn và thử thách. Tình yêu của họ có thể được hiểu là một cuộc phiêu lưu tìm kiếm ý nghĩa của cuộc sống, và chấp nhận rằng cuộc sống là ngắn ngủi và vô nghĩa.\n",
      "\n",
      "Tóm lại, \"Chiếc thuyền ngoài xa\" là một tác phẩm triết lý quan trọng, khám phá những vấn đề quan trọng về cuộc sống, tử thần, tình yêu, trách nhiệm và trách nhiệm cá nhân. Tác phẩm này có thể được hiểu là một cuộc phiêu lưu tìm kiếm ý nghĩa của cuộc sống, và chấp nhận rằng cuộc sống là ngắn ngủi và vô nghĩa.\n",
      "----------------------\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_content},\n",
    "    {\"role\": \"user\", \"content\": user_input},\n",
    "]\n",
    "\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id, # Sử dụng eos_token_id làm pad_token_id\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "# Decode câu trả lời - Chỉ lấy phần mới được generate\n",
    "response_start_index = inputs.shape[1]\n",
    "decoded_output = tokenizer.decode(outputs[0][response_start_index:], skip_special_tokens=True)\n",
    "print(\"\\n--- Model Response ---\")\n",
    "print(decoded_output)\n",
    "print(\"----------------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
