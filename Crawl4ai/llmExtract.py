import os
import asyncio
import json
import uuid
from dotenv import load_dotenv
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel
from crawl4ai.chunking_strategy import OverlappingWindowChunking
load_dotenv()

class RagBlock(BaseModel):
    tags: list[str]
    content: list[str]

async def main():
    gemini_token = os.getenv("GEMINI_API_KEY")
    if not gemini_token:
        raise ValueError("Missing GEMINI_API_KEY in .env file.")
    chunking_strategy = OverlappingWindowChunking(window_size=800, overlap=200)
    # Cáº¥u hÃ¬nh LLM strategy
    llm_strategy = LLMExtractionStrategy(
        llm_config=LLMConfig(
            provider="gemini/gemini-2.0-flash-exp",
            api_token=gemini_token
        ),
        extraction_type="schema",
        schema=RagBlock.model_json_schema(),
        instruction = (
            "TÃ¡ch ná»™i dung bÃ i viáº¿t thÃ nh cÃ¡c pháº§n theo tá»«ng chá»§ Ä‘á» rÃµ rÃ ng Ä‘á»ƒ phá»¥c vá»¥ há»‡ thá»‘ng RAG. "
            "Tráº£ vá» má»™t danh sÃ¡ch cÃ¡c object JSON theo máº«u:\n"
            "{ \"tags\": [\"chá»§ Ä‘á» cá»¥ thá»ƒ\"], \"content\": [\"ná»™i dung markdown\"] }\n"
            "KhÃ´ng tráº£ vá» chuá»—i Ä‘Æ¡n hoáº·c ná»™i dung ngoÃ i schema. KhÃ´ng tráº£ vá» quáº£ng cÃ¡o, link hoáº·c javascript, Ä‘iÃªÌ€u hÆ°Æ¡Ìng, biÌ€nh luÃ¢Ì£n, chuyÃªn muÌ£c khaÌc, liÃªn hÃªÌ£, giaÌ vaÌ€ng/ thiÌ£ trÆ°Æ¡Ì€ng, caÌc title chuyÃªn ngaÌ€nh khaÌc."
            "Táº¥t cáº£ tags vÃ  ná»™i dung pháº£i báº±ng tiáº¿ng Viá»‡t."
            "Quan troÌ£ng nhÃ¢Ìt laÌ€ Ä‘á»«ng tráº£ vá» chuá»—i JSON dáº¡ng string, chá»‰ tráº£ vá» list object JSON chuáº©n coÌ schema nhÆ° tÃ´i Ä‘aÌƒ nhÄƒÌc nhÆ° trÃªn."
        ),
        apply_chunking=True,
        chunk_token_threshold=2048,
        input_format="markdown",
        chunking_strategy=chunking_strategy,
        extra_args={"temperature": 0.2, "max_tokens": 2048}
    )

    config = CrawlerRunConfig(
        extraction_strategy=llm_strategy,
        cache_mode=CacheMode.BYPASS,
        css_selector=".detail-content", # Focus on this
    )

    async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler:
        result = await crawler.arun(
            url="https://bachnong.vn/tin-tuc/tin-tuc-nong-nghiep/top-9-benh-cay-trong-thuong-gap-phai-va-cach-khac-phuc.html",
            config=config
        )

        if not result.success:
            print("Crawl failed:", result.error_message)
            return
        print("Raw result")
        print(result)
        # Parse extracted content JSON
        try:
            blocks = json.loads(result.extracted_content)
        except json.JSONDecodeError as e:
            print("JSON decode failed:", e)
            return
        print("Raw from LLM")
        print(blocks)
        print("\nâœ… --- Extracted RAG-ready Markdown ---\n")
        with open("./data/rag_blocks_milvus.jsonl", "a", encoding="utf-8") as f:
            for block in blocks:
                try:
                    parsed = RagBlock(**block)
                    markdown_text = "\n".join(parsed.content)
                    entry = {
                        "id": str(uuid.uuid4()),
                        "content": markdown_text,
                        "metadata": {"tags": parsed.tags}
                    }
                    f.write(json.dumps(entry, ensure_ascii=False) + "\n")
                    print(f"### {' / '.join(parsed.tags)}\n{markdown_text}\n")
                except Exception as e:
                    print(f"Invalid block: {block}")
                    print("Error:", e)
        print("\nğŸ“Š --- Token Usage ---")
        llm_strategy.show_usage()

if __name__ == "__main__":
    asyncio.run(main())
